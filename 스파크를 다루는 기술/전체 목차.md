# 스파크를 다루는 기술
## 1부. 첫걸음
### 1장. 아파치 스파크 소개
#### 1.1 스파크란
##### 1.1.1 스파크가 가져온 혁명
##### 1.1.2 맵리듀스의 한계
##### 1.1.3 스파크가 가져다준 선물

#### 1.2 스파크를 구성하는 컴포넌트
##### 1.2.1 스파크 코어
##### 1.2.2 스파크 SQL
##### 1.2.3 스파크 스트리밍
##### 1.2.4 스파크 MLlib
##### 1.2.5 스파크 GraphX

#### 1.3 스파크 프로그램의 실행 과정

#### 1.4 스파크 생태계

#### 1.5 가상 머신 설정
##### 1.5.1 가상 머신 시작
##### 1.5.2 가상 머신 종료

#### 1.6 요약

### 2장. 스파크의 기초
#### 2.1 가상 머신 사용
##### 2.1.1 깃허브 저장소 복제
##### 2.1.2 자바 찾기
##### 2.1.3 가상 머신에 설치된 하둡 사용
##### 2.1.4 가상 머신에 설치된 스파크 살펴보기

#### 2.2 스파크 셸로 첫 스파크 프로그램 작성
##### 2.2.1 스파크 셸 시작
##### 2.2.2 첫 스파크 코드 예제
##### 2.2.3 RDD의 개념

#### 2.3 RDD의 기본 행동 연산자 및 변환 연산자
##### 2.3.1 map의 변환 연산자
##### 2.3.2 distinct와 flatMap 변환 연산자
##### 2.3.3 sample, take, takeSample 연산으로 RDD의 일부 요소 가져오기

#### 2.4 Double RDD 전용 함수
##### 2.4.1 double RDD 함수로 기초 통계량 계산
##### 2.4.2 히스토그램으로 데이터 분포 시각화
##### 2.4.3 근사 합계 및 평균 계산

#### 2.5 요약

### 3장. 스파크 애플리케이션 작성하기
#### 3.1 이클립스로 스파크 프로젝트 생성

#### 3.2 스파크 애플리케이션 개발
##### 3.2.1 깃허브 아카이브 데이터셋 준비
##### 3.2.2 JSON 로드
##### 3.2.3 이클립스에서 애플리케이션 실행
##### 3.2.4 데이터 집계
##### 3.2.5 분석 대상 제외
##### 3.2.6 공유 변수
##### 3.2.7 전체 데이터셋 사용

#### 3.3 애플리케이션 제출
##### 3.3.1 uberjar 빌드
##### 3.3.2 애플리케이션의 적응력 올리기
##### 3.3.3 spark-submit 사용

#### 3.4 요약


### 4장. 스파크 API 깊이 파헤치기
#### 4.1 Pair RDD 다루기
##### 4.1.1 Pair RDD 생성
##### 4.1.2 기본 Pair RDD 함수

#### 4.2 데이터 파티셔닝을 이해하고 데이터 셔플링 최소화
##### 4.2.1 스파크의 데이터 Partitioner
##### 4.2.2 불필요한 셔플링 줄이기
##### 4.2.3 RDD 파티션 변경
##### 4.2.4 파티션 단위로 데이터 매핑

#### 4.3 데이터 조인, 정렬, 그루핑
##### 4.3.1 데이터 조인
##### 4.3.2 데이터 정렬
##### 4.3.3 데이터 그루핑

#### 4.4 RDD 의존 관계
##### 4.4.1 RDD 의존 관계와 스파크 동작 매커니즘
##### 4.4.2 스파크의 스테이지와 태스크
##### 4.4.3 체크포인트로 RDD 계보 저장

#### 4.5 누적 변수와 공유 변수
##### 4.5.1 누적 변수로 실행자에서 데이터 가져오기
##### 4.5.2 공유 변수로 실행자에 데이터 전송

#### 4.6 요약


## 2부. 스파크 패밀리와 만남
### 5장. 스파크 SQL로 멋진 쿼리를 실행하자
#### 5.1 DataFrame 다루기
##### 5.1.1 RDD에서 DataFrame 생성
##### 5.1.2 기본 DataFrame API
##### 5.1.3 SQL 함수로 데이터에 연산 수행
##### 5.1.4 결측 값 다루기
##### 5.1.5 DataFrame을 RDD로 변환
##### 5.1.6 데이터 그루핑
##### 5.1.7 데이터 조인

#### 5.2 DataFrame을 넘어 Dataset으로

#### 5.3 SQL 명령
##### 5.3.1 데이터 카탈로그와 하이브 메타스토어
##### 5.3.2 SQL 쿼리 실행
##### 5.3.3 쓰리프트 서버로 스파크 SQL 접속

#### 5.4 DataFrame을 저장하고 불러오기
##### 5.4.1 기본 데이터 소스
##### 5.4.2 데이터 저장
##### 5.4.3 데이터 불러오기

#### 5.5 카탈리스트 최적화 엔진

#### 5.6 텅스텐 프로젝트의 스파크 성능 향상

#### 5.7 요약

### 6장. 스파크 스트리밍으로 데이터를 흐르게 하자
#### 6.1 스파크 스트리밍 애플리케이션 작성
##### 6.1.1 예제 애플리케이션
##### 6.1.2 스트리밍 컨텍스트 생성
##### 6.1.3 이산 스트림 생성
##### 6.1.4 이산 스트림 사용
##### 6.1.5 결과를 파일로 저장
##### 6.1.6 스트리밍 계산 작업의 시작과 종료
##### 6.1.7 시간에 따라 변화하는 계산 상태 저장
##### 6.1.8 윈도 연산으로 일정 시간 동안 유입된 데이터만 계산
##### 6.1.9 그 외 내장 입력 스트림

#### 6.2 외부 데이터 소스 사용
##### 6.2.1 카프카 시작
##### 6.2.2 카프카를 사용해 스트리밍 애플리케이션 개발

#### 6.3 스파크 스트리밍의 잡 성능
##### 6.3.1 성능 개선
##### 6.3.2 장애 내성

#### 6.4 정형 스트리밍
##### 6.4.1 스트리밍 DataFrame 생성
##### 6.4.2 스트리밍 데이터 출력
##### 6.4.3 스트리밍 실행 관리
##### 6.4.4 정형 스트리밍의 미래

#### 6.5 요약


### 7장. MLlib로 더 똑똑해지자
#### 7.1 머신 러닝의 개요
##### 7.1.1 머신 러닝의 정의
##### 7.1.2머신 러닝 알고리즘의 유형
##### 7.1.3 스파크를 활용한 머신 러닝

#### 7.2 스파크에서 선형 대수 연산 수행
##### 7.2.1 로컬 벡터와 로컬 행렬
##### 7.2.2 분산 행렬

#### 7.3 선형 회귀
##### 7.3.1 선형 회귀 소개
##### 7.3.2 단순 선형 회귀
##### 7.3.3 다중 선형 회귀로 모델 확장

#### 7.4 데이터 분석 및 준비
##### 7.4.1 데이터 분포 분석
##### 7.4.2 열 코사인 유사도 분석
##### 7.4.3 공분산 행렬 계산
##### 7.4.4 레이블 포인트로 변환
##### 7.4.5 데이터 분할
##### 7.4.6 특징 변수 스케일링 및 평균 정규화

#### 7.5 선형 회귀 모델 학습 및 활용
##### 7.5.1 목표 변수 값 예측
##### 7.5.2 모델 성능 평가
##### 7.5.3 모델 매개변수 해석
##### 7.5.4 모델의 저장 및 불러오기

#### 7.6 알고리즘 정확도 극대화
##### 7.6.1 적절한 이동 거리와 반복 횟수를 찾는 방법
##### 7.6.2 고차 다항식 추가
##### 7.6.3 편향-분산 상충 관계와 모델의 복잡도
##### 7.6.4 잔차 차트 그리기
##### 7.6.5 일반화를 사용해 과적합 방지
##### 7.6.6 k-겹 교차 검증

#### 7.7 알고리즘 성능 최적화
##### 7.7.1 미니배치 기반 확률적 경사 하강법
##### 7.7.2 LGBFGS 최적화

#### 7.8 요약

### 8장. 스파크 ML로 만드는 분류와 군집화
#### 8.1 스파크 ML 라이브러리
##### 8.1.1 변환자, 추정자, 평가자
##### 8.1.2 ML 매개변수
##### 8.1.3 ML 파이프라인

#### 8.2 로지스틱 회귀
##### 8.2.1 이진 로지스틱 회귀 모델
##### 8.2.2로지스틱 회귀에 필요한 데이터 준비
##### 8.2.3 로지스틱 회귀 모델 훈련
##### 8.2.4 분류 모델의 평가
##### 8.2.5 k-겹 교차 검증 수행
##### 8.2.6 다중 클래스 로지스틱 회귀


#### 8.3 의사결정 트리와 랜덤 포레스트
##### 8.3.1 의사 결정 트리
##### 8.3.2 랜덤 포레스트

#### 8.4 군집화
##### 8.4.1 k-평균 군집화

#### 8.5 요약



### 9장. 점을 연결하는 GraphX
#### 9.1 스파크의 그래프 연산
##### 9.1.1 GraphX API를 사용해 그래프 만들기
##### 9.1.2 그래프 변환

#### 9.2 그래프 알고리즘
##### 9.2.1 예제 데이터셋
##### 9.2.2 최단 경로 알고리즘
##### 9.2.3 페이지 랭크
##### 9.2.4 연결요소
##### 9.2.5 강연결요소

#### 9.3 A* 검색 알고리즘 구현
##### 9.3.1 A* 알고리즘 이해
##### 9.3.2 A* 알고리즘 구현
##### 9.3.3 구현된 알고리즘 테스트

#### 9.4 요약


## 3부. 스파크 옵스
### 10장. 스파크 클러스터 구동
#### 10.1 스파크 런타임 아키텍처의 개요
##### 10.1.1 스파크 런타임 컴포넌트
##### 10.1.2 스파크 클러스터 유형

#### 10.2 잡 스케줄링과 리소스 스케줄링
##### 10.2.1 클러스터 리소스 스케줄링
##### 10.2.2 스파크 잡 스케줄링
##### 10.2.3 데이터 지역성
##### 10.2.4 스파크의 메모리 스케줄링

#### 10.3 스파크 설정
##### 10.3.1 스파크 환경 설정 파일
##### 10.3.2 명령줄 매개변수
##### 10.3.3 시스템 환경 변수
##### 10.3.4 프로그램 코드로 환경 설정
##### 10.3.5 master 매개변수
##### 10.3.6 설정된 매개변수 조회

#### 10.4 스파크 웹 UI
##### 10.4.1 Jobs 페이지
##### 10.4.2 Stages 페이지
##### 10.4.3 Storage 페이지
##### 10.4.4 Environment 페이지
##### 10.4.5 Executors 페이지

#### 10.5 로컬 머신에서 스파크 실행
##### 10.5.1 로컬 모드
##### 10.5.2 로컬 클러스터 모드

#### 10.6 요약


### 11장. 스파크 자체 클러스터
#### 11.1 스파크 자체 클러스터의 컴포넌트

#### 11.2 스파크 자체 클러스터 시작
##### 11.2.1 셸 스크립트로 클러스터 시작
##### 11.2.2 수동으로 클러스터 시작
##### 11.2.3 스파크 프로세스 조회
##### 11.2.4 마스터 고가용성 및 복구 기능

#### 11.3 스파크 자체 클러스터의 웹 UI

#### 11.4 스파크 자체 클러스터에서 애플리케이션 실행
##### 11.4.1 드라이버의 위치
##### 11.4.2 실행자 개수 지정
##### 11.4.3 추가 클래스패스 항목 및 파일 지정
##### 11.4.4 애플리케이션 강제 종료
##### 11.4.5 애플리케이션 자동 재시작

#### 11.5 스파크 히스토리 서버와 이벤트 로깅

#### 11.6 아마존 EC2에서 스파크 실행
##### 11.6.1 사전 준비
##### 11.6.2 EC2 기반 스파크 자체 클러스터 생성
##### 11.6.3 EC2 클러스터 사용
##### 11.6.4 클러스터 제거

#### 11.7 요약


### 12장. YARN 클러스터와 메소스 클러스터
#### 12.1 YARN에서 스파크 실행
##### 12.1.1 YARN 아키텍처
##### 12.1.2 YARN 설치, 구성 및 시작
##### 12.1.3 YARN의 리소스 스케줄링
##### 12.1.4 YARN에 스파크 애플리케이션 제출
##### 12.1.5 YARN에서 스파크 설정
##### 12.1.6 스파크 잡에 할당할 리소스 설정
##### 12.1.7 YARN  UI
##### 12.1.8 YARN에서 로그 조회
##### 12.1.9 보안 관련 사항
##### 12.1.10 동적 리소스 할당

#### 12.2 메소스에서 스파크 실행
##### 12.2.1 메소스 아키텍처
##### 12.2.2 메소스 설치 및 설정
##### 12.2.3 메소스 웹 UI
##### 12.2.4 메소스의 리소스 스케줄링
##### 12.2.5 메소스에 스파크 애플리케이션 제출
##### 12.2.6 도커로 스파크 실행

#### 12.3 요약

## 4부. 스파크의 활용
### 13장. 실용 예제_ 실시간 대시보드를 구현하자
#### 13.1 예제 애플리케이션 소개
##### 13.1.1 예제 시나리오
##### 13.1.2 예제 애플리케이션의 컴포넌트

#### 13.2 애플리케이션 실행
##### 13.2.1 가상 머신에서 애플리케이션 시작
##### 13.2.2 애플리케이션을 수동으로 시작

#### 13.3 소스 코드 이해
##### 13.3.1 KafkaLogsSimulator 프로젝트
##### 13.3.2 StreamingLogAnalyzer 프로젝트
##### 13.3.3 WebStatsDashboard 프로젝트
##### 13.3.4  프로젝트 빌드

#### 13.4 요약

### 14장. 스파크와 H2O를 활용한 딥러닝
#### 14.1 딥러닝의 개요

#### 14.2 스파크에서 H2O 사용
##### 14.2.1 H2O의 개요
##### 14.2.2 스파크에서 스파클링 워터 시작
##### 14.2.3 H2O 클러스터 시작
##### 14.2.4 플로 UI에 접속

#### 14.3 H2O의 딥러닝을 사용한 회귀 예측
##### 14.3.1 데이터를 HwO 프레임으로 로드
##### 14.3.2 플로 UI로 딥러닝 모델 구축 및 평가
##### 14.3.3 스파클링 워터 API로 딥러닝 모델 구축 및 평가

#### 14.4 H2O의 딥러닝을 사용한 분류 예측
##### 14.4.1 데이터 로드 및 분할
##### 14.4.2 플로 UI로 모델 구축
##### 14.4.3 스파클링 워터 API로 모델 구축
##### 14.4.4 H2O 클러스터 중지

#### 14.5 요약

### 부록 A. 아파치 스파크 설치
#### A.1 사전 준비: JDK 설치
#### A.2 JAVA_HOME 환경 변수 설정
#### A.3 스파크 내려받기, 설치, 설정
#### A.4 스파크 셸

### 부록 B. 맵리듀스


### 부록 C. 선형 대수학 입문