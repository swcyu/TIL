#### 1.1 스파크란
##### 1.1.1 스파크가 가져온 혁명
##### 1.1.2 맵리듀스의 한계
##### 1.1.3 스파크가 가져다준 선물

#### 1.2 스파크를 구성하는 컴포넌트
##### 1.2.1 스파크 코어
##### 1.2.2 스파크 SQL
##### 1.2.3 스파크 스트리밍
##### 1.2.4 스파크 MLlib
##### 1.2.5 스파크 GraphX

#### 1.3 스파크 프로그램의 실행 과정

#### 1.4 스파크 생태계

#### 1.5 가상 머신 설정
##### 1.5.1 가상 머신 시작
##### 1.5.2 가상 머신 종료

#### 1.6 요약

---
#### 1.1 스파크란
- 아파치 하둡
	- 분산 컴퓨팅용 자바 기반 오픈 소스 프레임워크
	- 구성 = HDFS(하둡 분산 파일 시스템, Hadoop Distributed File System) + 맵리듀스 처리 엔진
- 아파치 스파크
	- 범용 분산 컴퓨팅 플랫폼
- 스파크가 적합한 작업
	- 일괄 처리 작업
	- 온라인 분석 처리 작업 (OLAP, OnLine Analytical Processing)
		- 데이터 마이닝
- 스파크가 부적합한 작업
	- OLTP 어플리케이션
		- 온라인 트랜잭션 처리 (OnLine Transaction Processing)
		- 대량의 원자성(atomicity) 트랜잭션을 빠르게 처리해야 하는 작업
	- 단일 머신에서 충분히 처리할 수 있는 데이터 셋


##### 1.1.1 스파크가 가져온 혁명
- 하둡
	- 해결한 세 가지 문제
		- 병렬처리 (parallelization)
		- 데이터 분산 (distribution)
		- 장애 대응 (fault tolerance)
	- 하둡 클러스터
		- 상용 하드웨어 사용해 비교적 손쉽게 구축할 수 있다


##### 1.1.2 맵리듀스의 한계
- 맵리듀스의 한계
	- 반복 알고리즘에는 본질적으로 맞지 않다
		- 맵리듀스 잡의 결과를 다른 잡에서 사용하려면 먼저 결과를 HDFS에 저장해야
	- 맵 리듀스 API 사용 어려운 수도 있음
	- 좁은 범위의 하위 레벨 프레임워크의 한계
		- 주변 도구 필요: 데이터 내보내기, 가져오기 등 


##### 1.1.3 스파크가 가져다준 선물
- 스파크의 핵심
	- 인-메모리 실행방식: 데이터를 메모리에 캐시로 저장

###### 1.1.3.1 사용 편의성
###### 1.1.3.2 통합 플랫폼
###### 1.1.3.3 안티 패턴



#### 1.2 스파크를 구성하는 컴포넌트
##### 1.2.1 스파크 코어


##### 1.2.2 스파크 SQL


##### 1.2.3 스파크 스트리밍


##### 1.2.4 스파크 MLlib


##### 1.2.5 스파크 GraphX



#### 1.3 스파크 프로그램의 실행 과정



#### 1.4 스파크 생태계
- 스파크가 대체할 수 있는 하둡 생태계의 도구
	- 아파치 지라프(Giraph) --> 스파크 GraphX
	- 아파치 머하웃(Mahout) --> 스파크 MLlib
	- 아파치 스톰(Storm) --> 스파크 스트리밍
	- 아파치 피그(Pig), 아파치 스쿱(Sqoop) --> 스파크 코어, 스파크 SQL
- 스파크가 대체할 수 없는 하둡 생태계의 도구
	- 인프라 및 관리 도구
		- 아파치 우지(Oozie): 다른 여러 유형의 하둡 잡 스케줄링
		- HBase: 다른 성격의 대규모 분산 데이터베이스
		- 주키퍼(Zookeeper): 고성능 코디네이션 서비스
			- 분산 동기화, 네이밍, 그룹 서비스 프로비저닝 등 구현
- 스파크와 공존할 수 있는
	- 아파치 임팔라(Impala), 드릴(Drill)
		- 스파크의 기능을 풍성하게 할 경쟁 프레임워크
			- 스파크 코어, 스파크 SQL 기능 포괄

- 스토리지
	- HDFS
	- 아마존 S3 버킷
	- 일반 파일 시스템
	- 메모리 중심 분산 파일 시스템 
		- 알루시오 (Alluxio)
		- GlusterFS
- 클러스터 매니저
	- YARN
	- 아파치 메소스
		- 분산 리소스를 추상화한 고급 분산 시스템 커널
		- 확장성: 장애 내성 유지하면서 클러스터 노드를 수만 개로 늘릴 수 있음
	- 스파크 자체 클러스터


#### 1.5 가상 머신 설정
##### 1.5.1 가상 머신 시작
```plain text

1. Vagrant 다운받고 설치 (amd64)
2. D드라이브에 spark-in-action 폴더 만들기
3. 해당 위치(D:\spark-in-action)에 Powershell 실행해서 파일 다운받기
	1. 폴더 경로 창에 powershell 입력
	2. wget -Uri https://raw.githubusercontent.com/spark-in-action/first-edition/master/spark-in-action-box.json -OutFile spark-in-action-box.json
4. vagrant box add --name manning/spark-in-action spark-in-action-box.json (15분소요)
5. vagrant init manning/spark-in-action : 가상머신 초기화
6. vagrant up : 가상 머신 시작 ()
7. vagrant halt : 가상 머신 중지


- 가상머신 IP 주소 : 192.168.33.10
- 가상머신 접속하기
	- ssh -p 2222 spark@127.0.0.1
	- spark
```


##### 1.5.2 가상 머신 종료



#### 1.6 요약
- 스파크 - 단일 프레임 워크에서 통합 플랫폼 제공
	- 일괄 처리 기능
	- 실시간 데이터 처리 기능
	- SQL과 유사한 정형 데이터 처리 기능
	- 그래프 알고리즘 및 머신 러닝 지원
- 스파크가 적합하지 않은 작업
	- 소량의 데이터셋을 처리하는 작업
	- OLTP 애플리케이션
- 스파크 주요 컴포넌트
	- 스파크 코어
	- 스파크 SQL
	- 스파크 스트리밍
	- 스파크 MLlib
	- 스파크 GraphX

